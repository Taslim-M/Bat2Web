{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pi/.local/lib/python3.7/site-packages/numba/core/errors.py:144: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/home/pi/.local/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/pi/.local/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF LITE MODEL LOADING\n",
    "interpreter = tf.lite.Interpreter(model_path=\"mel_smote_v1_iter_1.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "# input shape to ensure correct dimension\n",
    "input_shape = input_details[0]['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcm2835 HDMI 1: - (hw:0,0)\n",
      "iRig Mic Cast HD: USB Audio (hw:1,0)\n",
      "sysdefault\n",
      "lavrate\n",
      "samplerate\n",
      "speexrate\n",
      "pulse\n",
      "upmix\n",
      "vdownmix\n",
      "dmix\n",
      "default\n",
      "Mic found at 1\n"
     ]
    }
   ],
   "source": [
    "#PREPARE MIC SETTINGS\n",
    "p = pyaudio.PyAudio()\n",
    "for ii in range(p.get_device_count()):\n",
    "    print(p.get_device_info_by_index(ii).get('name'))\n",
    "    if p.get_device_info_by_index(ii).get('name') == 'iRig Mic Cast HD: USB Audio (hw:1,0)':\n",
    "        index = ii\n",
    "print('Mic found at',index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRE PROCESS AUDIO AND RUN MODEL\n",
    "def process(final_arr, start_time, fig):\n",
    "    #PreProcess\n",
    "    spec = librosa.feature.melspectrogram(y=final_arr, sr=sr, n_fft = int(window_width*sr), hop_length =int(sliding*sr), fmax=sr/2)\n",
    "    #Convert amplitude to decibels\n",
    "    db_spec = librosa.power_to_db(spec, ref=np.max)\n",
    "    ax = plt.axes()\n",
    "    librosa.display.specshow(db_spec, sr=sr, hop_length =int(sliding*sr),fmax=sr/2)\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout(pad=0)\n",
    "    ax.margins(0)\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    image_from_plot = np.float32(image_from_plot) #For Model Compatibility\n",
    "    image_from_plot = np.expand_dims(image_from_plot, axis = 0)\n",
    "    plt.clf()\n",
    "    #Record total preprocessing time\n",
    "    pre_proc_time.append(time.time() - start) \n",
    "    #End PreProcess\n",
    "    \n",
    "    #Call TF LITE Model\n",
    "    start_eval = time.time()\n",
    "    interpreter.set_tensor(input_details[0]['index'], image_from_plot)\n",
    "    interpreter.invoke()  # runs the test\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    #Record model inference time\n",
    "    infer_time.append(time.time() - start_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_1 = pyaudio.paInt16 # 24-bit resolution but using 16\n",
    "chans = 1 # 1 channel\n",
    "samp_rate = 44100 # 44.1kHz sampling rate\n",
    "chunk = 4096 # 2^12 samples for buffer\n",
    "record_secs = 3 # seconds to record\n",
    "dev_index = index # device index found by p.get_device_info_by_index(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 170x112 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MEL CONFIGS\n",
    "window_width =  0.025 #25 ms  window size \n",
    "sliding = 0.01 #10ms stride \n",
    "sr = samp_rate\n",
    "\n",
    "fig = plt.figure(figsize=(1.70, 1.12), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc_time = [] #Record prepreoccsing times\n",
    "infer_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio = pyaudio.PyAudio() # create pyaudio instantiation\n",
    "\n",
    "# create pyaudio stream\n",
    "stream = audio.open(format = form_1,rate = samp_rate,\n",
    "                    channels = chans, \n",
    "                    input_device_index = dev_index,\n",
    "                    input = True,\n",
    "                    frames_per_buffer=chunk)\n",
    "#Repeat for 10 minutes (200 samples 3*200 = 600 seconds)\n",
    "for iteration in range(200):\n",
    "    frames = []\n",
    "\n",
    "    # loop through stream and append audio chunks to frame array\n",
    "    for ii in range(0,int((samp_rate/chunk)*record_secs)):\n",
    "        data = stream.read(chunk,exception_on_overflow = False)\n",
    "        frames.append(np.fromstring(data,dtype=np.float16))\n",
    "\n",
    "    #Begin processing\n",
    "    start = time.time()\n",
    "    final_arr = np.array(frames).ravel()\n",
    "    np.nan_to_num(final_arr,copy=False)\n",
    "    process(final_arr, start, fig)\n",
    "# stop the stream, close it, and terminate the pyaudio instantiation\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean preprocessing time 0.22687649488449096\n",
      "std 0.019475593118999986\n"
     ]
    }
   ],
   "source": [
    "ppt = np.array(pre_proc_time)\n",
    "print('mean preprocessing time',ppt.mean())\n",
    "print('std',ppt.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean inferance time 0.018013337850570677\n",
      "std 0.0021230191626599388\n"
     ]
    }
   ],
   "source": [
    "it = np.array(infer_time)\n",
    "print('mean inferance time',it.mean())\n",
    "print('std',it.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean total time 0.24488983273506165\n",
      "std 0.01971057829559917\n"
     ]
    }
   ],
   "source": [
    "total_time = it+ ppt\n",
    "\n",
    "print('mean total time',total_time.mean())\n",
    "print('std',total_time.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
