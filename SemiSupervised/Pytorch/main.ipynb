{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!coding:utf-8\n",
    "import os\n",
    "import torch\n",
    "from itertools import cycle\n",
    "\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "from utils import datasets\n",
    "from utils.ramps import exp_warmup\n",
    "from utils.config import parse_commandline_args\n",
    "from utils.data_utils import DataSetWarpper\n",
    "from utils.data_utils import TwoStreamBatchSampler\n",
    "from utils.data_utils import TransformTwice as twice\n",
    "from architectures.arch import arch\n",
    "\n",
    "from trainer import *\n",
    "build_model = {\n",
    "    'mtv1': MeanTeacherv1.Trainer,\n",
    "    'mtv2': MeanTeacherv2.Trainer,\n",
    "    'piv1': PIv1.Trainer,\n",
    "    'piv2': PIv2.Trainer,\n",
    "    'vatv1': VATv1.Trainer,\n",
    "    'vatv2': VATv2.Trainer,\n",
    "    'epslab2013v1': ePseudoLabel2013v1.Trainer,\n",
    "    'epslab2013v2': ePseudoLabel2013v2.Trainer,\n",
    "    'ipslab2013v1': iPseudoLabel2013v1.Trainer,\n",
    "    'ipslab2013v2': iPseudoLabel2013v2.Trainer,\n",
    "    'etempensv1': eTempensv1.Trainer,\n",
    "    'etempensv2': eTempensv2.Trainer,\n",
    "    'itempensv1': iTempensv1.Trainer,\n",
    "    'itempensv2': iTempensv2.Trainer,\n",
    "    'ictv1': ICTv1.Trainer,\n",
    "    'ictv2': ICTv2.Trainer,\n",
    "    'mixmatch': MixMatch.Trainer,\n",
    "    'ifixmatch': iFixMatch.Trainer,\n",
    "    'efixmatch': eFixMatch.Trainer,\n",
    "    'emixpslabv1': eMixPseudoLabelv1.Trainer,\n",
    "    'emixpslabv2': eMixPseudoLabelv2.Trainer,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders_v1(trainset, evalset, label_idxs, unlab_idxs,\n",
    "                      num_classes,\n",
    "                      config):\n",
    "    if config.data_twice: trainset.transform = twice(trainset.transform)\n",
    "    if config.data_idxs: trainset = DataSetWarpper(trainset, num_classes)\n",
    "    ## two-stream batch loader\n",
    "    batch_size = config.sup_batch_size + config.usp_batch_size\n",
    "    batch_sampler = TwoStreamBatchSampler(\n",
    "        unlab_idxs, label_idxs, batch_size, config.sup_batch_size)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                               batch_sampler=batch_sampler,\n",
    "                                               num_workers=config.workers,\n",
    "                                               pin_memory=True)\n",
    "    ## test batch loader\n",
    "    eval_loader = torch.utils.data.DataLoader(evalset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=2*config.workers,\n",
    "                                              pin_memory=True,\n",
    "                                              drop_last=False)\n",
    "    return train_loader, eval_loader\n",
    "\n",
    "\n",
    "def create_loaders_v2(trainset, evalset, label_idxs, unlab_idxs,\n",
    "                      num_classes,\n",
    "                      config):\n",
    "    if config.data_twice: trainset.transform = twice(trainset.transform)\n",
    "    if config.data_idxs: trainset = DataSetWarpper(trainset, num_classes)\n",
    "    ## supervised batch loader\n",
    "    label_sampler = SubsetRandomSampler(label_idxs)\n",
    "    label_batch_sampler = BatchSampler(label_sampler, config.sup_batch_size,\n",
    "                                       drop_last=True)\n",
    "    label_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_sampler=label_batch_sampler,\n",
    "                                          num_workers=config.workers,\n",
    "                                          pin_memory=True)\n",
    "    ## unsupervised batch loader\n",
    "    if not config.label_exclude: unlab_idxs += label_idxs\n",
    "    unlab_sampler = SubsetRandomSampler(unlab_idxs)\n",
    "    unlab_batch_sampler = BatchSampler(unlab_sampler, config.usp_batch_size,\n",
    "                                       drop_last=True)\n",
    "    unlab_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_sampler=unlab_batch_sampler,\n",
    "                                          num_workers=config.workers,\n",
    "                                          pin_memory=True)\n",
    "    ## test batch loader\n",
    "    eval_loader = torch.utils.data.DataLoader(evalset,\n",
    "                                           batch_size=config.sup_batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=2*config.workers,\n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=False)\n",
    "    return label_loader, unlab_loader, eval_loader\n",
    "\n",
    "\n",
    "def create_optim(params, config):\n",
    "    if config.optim == 'sgd':\n",
    "        optimizer = optim.SGD(params, config.lr,\n",
    "                              momentum=config.momentum,\n",
    "                              weight_decay=config.weight_decay,\n",
    "                              nesterov=config.nesterov)\n",
    "    elif config.optim == 'adam':\n",
    "        optimizer = optim.Adam(params, config.lr)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lr_scheduler(optimizer, config):\n",
    "    if config.lr_scheduler == 'cos':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                   T_max=config.epochs,\n",
    "                                                   eta_min=config.min_lr)\n",
    "    elif config.lr_scheduler == 'multistep':\n",
    "        if config.steps is None: return None\n",
    "        if isinstance(config.steps, int): config.steps = [config.steps]\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer,\n",
    "                                             milestones=config.steps,\n",
    "                                             gamma=config.gamma)\n",
    "    elif config.lr_scheduler == 'exp-warmup':\n",
    "        lr_lambda = exp_warmup(config.rampup_length,\n",
    "                               config.rampdown_length,\n",
    "                               config.epochs)\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "    elif config.lr_scheduler == 'none':\n",
    "        scheduler = None\n",
    "    else:\n",
    "        raise ValueError(\"No such scheduler: {}\".format(config.lr_scheduler))\n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    print(config)\n",
    "    print(\"pytorch version : {}\".format(torch.__version__))\n",
    "    ## create save directory\n",
    "    if config.save_freq!=0 and not os.path.exists(config.save_dir):\n",
    "        os.makedirs(config.save_dir)\n",
    "    ## prepare data\n",
    "    dconfig   = datasets.load[config.dataset](config.num_labels)\n",
    "    if config.model[-1]=='1':\n",
    "        loaders = create_loaders_v1(**dconfig, config=config)\n",
    "    elif config.model[-1]=='2' or config.model[-5:]=='match':\n",
    "        loaders = create_loaders_v2(**dconfig, config=config)\n",
    "    else:\n",
    "        raise ValueError('No such model: {}'.format(config.model))\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ## prepare architecture\n",
    "    net = arch[config.arch](dconfig['num_classes'], config.drop_ratio)\n",
    "    net = net.to(device)\n",
    "    optimizer = create_optim(net.parameters(), config)\n",
    "    scheduler = create_lr_scheduler(optimizer, config)\n",
    "\n",
    "    ## run the model\n",
    "    MTbased = set(['mt', 'ict'])\n",
    "    if config.model[:-2] in MTbased or config.model[-5:]=='match':\n",
    "        net2 = arch[config.arch](dconfig['num_classes'], config.drop_ratio)\n",
    "        net2 = net2.to(device)\n",
    "        trainer = build_model[config.model](net, net2, optimizer, device, config)\n",
    "    else:\n",
    "        trainer = build_model[config.model](net, optimizer, device, config)\n",
    "    trainer.loop(config.epochs, *loaders, scheduler=scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = {\n",
    "     # Log and save\n",
    "    \"print_freq\": 20,\n",
    "    \"save_freq\": 0,\n",
    "    \"save_dir\": './checkpoints',\n",
    "     # Data\n",
    "    \"dataset\": 'bat_data',\n",
    "    \"workers\": 4,\n",
    "    \"num_labels\": 4000,\n",
    "    \"sup_batch_size\": 100,\n",
    "    \"usp_batch_size\": 100,\n",
    "     # Data pre-processing\n",
    "    \"data_twice\": False,\n",
    "    \"data_idxs\": False,\n",
    "    \"label_exclude\": False,\n",
    "     # Architecture\n",
    "    \"arch\": 'cnn13',\n",
    "    \"model\": 'ifixmatch',\n",
    "    \"drop_ratio\": 0,\n",
    "    # Optimization\n",
    "    \"epochs\": 400,\n",
    "    \"optim\": 'sgd',\n",
    "    \"momentum\": 0.9,\n",
    "    \"nesterov\": True,\n",
    "    \"weight_decay\": 5e-4,\n",
    "     # LR schecular\n",
    "    \"lr\": 0.1,\n",
    "    \"lr_scheduler\": 'cos',\n",
    "    \"min_lr\": 1e-4,\n",
    "    \"steps\": None,\n",
    "    \"gamma\": None,\n",
    "    \"rampup_length\": 80,\n",
    "    \"rampdown_length\": 50,\n",
    "    # Pseudo-Label 2013\n",
    "    \"t1\": None,\n",
    "    \"t2\": None,\n",
    "    \"soft\": None,\n",
    "    # VAT\n",
    "    \"xi\": None,\n",
    "    \"eps\": None,\n",
    "    \"n_power\": None,\n",
    "    # Fixmatch\n",
    "    \"threshold\": 0.95,\n",
    "     # MeanTeacher-based method\n",
    "    \"ema_decay\": 0.97,\n",
    "    # Mixup-based method\n",
    "    \"mixup_alpha\": None,\n",
    "     # Opt for loss\n",
    "    \"usp_weight\": 1.0,\n",
    "    \"weight_rampup\": 30,\n",
    "    \"ent_weight\": None\n",
    "}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(arch='cnn13', data_idxs=False, data_twice=False, dataset='bat_data', drop_ratio=0, ema_decay=0.97, ent_weight=None, epochs=400, eps=None, gamma=None, label_exclude=False, lr=0.1, lr_scheduler='cos', min_lr=0.0001, mixup_alpha=None, model='ifixmatch', momentum=0.9, n_power=None, nesterov=True, num_labels=4000, optim='sgd', print_freq=20, rampdown_length=50, rampup_length=80, save_dir='./checkpoints', save_freq=0, soft=None, steps=None, sup_batch_size=100, t1=None, t2=None, threshold=0.95, usp_batch_size=100, usp_weight=1.0, weight_decay=0.0005, weight_rampup=30, workers=4, xi=None)\n",
      "pytorch version : 1.6.0\n",
      "FixMatch\n",
      "------ Training epochs: 0 ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team9/anaconda3/envs/gpu/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d5da1b3c2017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-8f46bdec164d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/bat_project/SemiSupervised/Pytorch/trainer/iFixMatch.py\u001b[0m in \u001b[0;36mloop\u001b[0;34m(self, epochs, label_data, unlab_data, test_data, scheduler)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------ Training epochs: {} ------\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlab_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------ Testing epochs: {} ------\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bat_project/SemiSupervised/Pytorch/trainer/iFixMatch.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, label_loader, unlab_loader, print_freq)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlab_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bat_project/SemiSupervised/Pytorch/trainer/iFixMatch.py\u001b[0m in \u001b[0;36mtrain_iteration\u001b[0;34m(self, label_loader, unlab_loader, print_freq)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloop_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlab_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlab_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlab_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mlabel_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrong_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    config = SimpleNamespace(**Config)\n",
    "    run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
