{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D, GlobalAveragePooling2D, Input,AveragePooling2D, BatchNormalization, LeakyReLU, SpatialDropout2D, Lambda, Reshape, Conv2DTranspose, Conv2D\n",
    "\n",
    "# from keras.applications import MobileNet\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2, InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from collections import defaultdict,Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from PIL import Image\n",
    "\n",
    "from kerastuner import HyperModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Loading and Preparation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeciesCode(x):\n",
    "    part = x.split('_')\n",
    "    if part[0] == 'ASETRI':\n",
    "        return 0\n",
    "    elif part[0] == 'EPTBOT':\n",
    "        return 1\n",
    "    elif part[0] == 'MYOEMA':\n",
    "        return 2\n",
    "    elif part[0] == 'PIPKUH':\n",
    "        return 3\n",
    "    elif part[0] == 'RHIMUS':\n",
    "        return 4\n",
    "    elif part[0] == 'RHYNAS':\n",
    "        return 5\n",
    "    elif part[0] == 'ROUAEG':\n",
    "        return 6\n",
    "    elif part[0] == 'TAPPER':\n",
    "        return 7\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "def getSpecies(x):\n",
    "    if x == 0:\n",
    "        return 'A.tridens'\n",
    "    elif x == 1:\n",
    "        return 'E.bottae'\n",
    "    elif x == 2:\n",
    "        return 'M.emarginatus'\n",
    "    elif x == 3:\n",
    "        return 'P.kuhli'\n",
    "    elif x == 4:\n",
    "        return 'R.muscatellum'\n",
    "    elif x == 5:\n",
    "        return 'R.nasutus'\n",
    "    elif x == 6:\n",
    "        return 'R.aegyptius'\n",
    "    elif x == 7:\n",
    "        return 'T.perforatus'\n",
    "    elif x == 8:\n",
    "        return 'No_Bat'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "    \n",
    "def generate_actual_predicted(Y_pred, X_test, Y_test): \n",
    "    predicted = list()\n",
    "    for i in range(len(Y_pred)):\n",
    "        predicted.append(np.argmax(Y_pred[i]))\n",
    "        \n",
    "    actual = list()\n",
    "    for i in range(len(Y_test)):\n",
    "        actual.append(np.argmax(Y_test[i]))\n",
    "        \n",
    "    return actual, predicted\n",
    "\n",
    "\n",
    "\n",
    "# # Ytrain in Onehot encoded form \n",
    "# def makeOverSamplesSMOTE(X_train,Y_train):\n",
    "    \n",
    "#     Y_train_labelled=[]\n",
    "#     X_dims=X_train.shape\n",
    "\n",
    "#     for i in range(len(Y_train)):\n",
    "#         Y_train_labelled.append(np.argmax(Y_train[i]))\n",
    "\n",
    "#     print('Original trainingset shape %s' %  [(getSpecies(k),v) for k,v in Counter(Y_train_labelled).items()])       \n",
    "#     X_train= X_train.reshape(-1,X_dims[1]*X_dims[2]*X_dims[3])\n",
    "\n",
    "#     sm = SMOTE(sampling_strategy='all',k_neighbors=5)\n",
    "#     X_train, Y_train_labelled = sm.fit_resample(X_train, Y_train_labelled)\n",
    "\n",
    "\n",
    "#     print('Resampled training set shape %s' % [(getSpecies(k),v) for k,v in Counter(Y_train_labelled).items()])\n",
    "\n",
    "\n",
    "#     #reshape X_all\n",
    "#     X_train= X_train.reshape(-1,X_dims[1],X_dims[2],X_dims[3])\n",
    "\n",
    "#     # update Y_train\n",
    "#     Y_train= np_utils.to_categorical(Y_train_labelled, num_classes=9)\n",
    "\n",
    "#     print(\"After OverSampling\\nX_train: shape= \",X_train.shape)\n",
    "#     print(\"Y_train: shape= \",Y_train.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "#     return(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RHIMUS    1665\n",
       "TAPPER     403\n",
       "PIPKUH     299\n",
       "RHYNAS     269\n",
       "EPTBOT     124\n",
       "ROUAEG     121\n",
       "MYOEMA     112\n",
       "ASETRI      25\n",
       "Name: Species, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_folder_path = '../data/SpectogramInitial'\n",
    "file_paths = [f for f in os.listdir(image_folder_path)]\n",
    "species = []\n",
    "for file_name in file_paths:\n",
    "    sp = file_name.split('_')\n",
    "    species.append(sp[0])\n",
    "\n",
    "df = pd.DataFrame(species, columns=['Species'])\n",
    "df['Species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all = []\n",
    "Y_all = []\n",
    "\n",
    "image_folder_path = '../data/SpectogramInitial'\n",
    "file_paths = [f for f in os.listdir(image_folder_path)]\n",
    "\n",
    "for file_name in file_paths:\n",
    "    spectrogram = Image.open(image_folder_path + '/' + file_name)\n",
    "    spectrogram = spectrogram.convert('RGB')\n",
    "    spectrogram = spectrogram.resize((168, 112))  \n",
    "    spectrogram = np.array(spectrogram)\n",
    "    #spectrogram = np.expand_dims(spectrogram, axis=2) \n",
    "    X_all.append(spectrogram)\n",
    "    Y_all.append(getSpeciesCode(file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Noise\n",
    "image_folder_path = '../data/noise'\n",
    "file_paths = [f for f in os.listdir(image_folder_path)]\n",
    "\n",
    "for file_name in file_paths:\n",
    "    spectrogram = Image.open(image_folder_path + '/' + file_name)\n",
    "    spectrogram = spectrogram.convert('RGB')\n",
    "    spectrogram = spectrogram.resize((168, 112))  \n",
    "    spectrogram = np.array(spectrogram)\n",
    "    #spectrogram = np.expand_dims(spectrogram, axis=2) \n",
    "    X_all.append(spectrogram)\n",
    "    Y_all.append(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3426, 112, 168, 3)\n",
      "(3426,)\n"
     ]
    }
   ],
   "source": [
    "X_all = np.array(X_all)\n",
    "Y_all = np.array(Y_all)\n",
    "\n",
    "print(X_all.shape)\n",
    "print(Y_all.shape)\n",
    "\n",
    "X_all = X_all.astype('float32')\n",
    "X_all = (X_all - 127.5) / 127.5\n",
    "\n",
    "Y_all = np_utils.to_categorical(Y_all, num_classes=9) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train /test= 80/20% split\n",
    "# Data is stratified\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size=0.2, random_state = 245, stratify=Y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2740,)\n",
      "Frequency of unique values of the said array:\n",
      "[[   0    1    2    3    4    5    6    7    8]\n",
      " [  20   99   90  239 1332  215   97  322  326]]\n"
     ]
    }
   ],
   "source": [
    "y_arr =np.argmax(Y_train, axis=1)\n",
    "print(y_arr.shape)\n",
    "unique_elements, counts_elements = np.unique(y_arr, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> V1 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom activation function\n",
    "def custom_activation(output):\n",
    "    logexpsum = backend.sum(backend.exp(output), axis=-1, keepdims=True)\n",
    "    result = logexpsum / (logexpsum + 1.0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone supervised and unsupervised discriminator models\n",
    "def define_discriminator(n_classes=9):\n",
    "    in_image = Input(shape=(112,168,3))\n",
    "    lay = Convolution2D(filters=56,kernel_size=(7,7),strides=(2,2),padding='same')(in_image)\n",
    "    lay = BatchNormalization()(lay)\n",
    "    lay = Activation('relu')(lay)\n",
    "    lay = MaxPooling2D(pool_size=(2,2),strides=2,padding='same')(lay)\n",
    "    lay = Dropout(0.3)(lay)\n",
    "\n",
    "    lay = Convolution2D(filters=72,kernel_size=(5,5),strides=(2,2),padding='same')(lay)\n",
    "    lay = BatchNormalization()(lay)\n",
    "    lay = Activation('relu')(lay)\n",
    "    lay = AveragePooling2D(pool_size=(2,2),strides=2,padding='same')(lay)\n",
    "    lay = Dropout(0.3)(lay)\n",
    "\n",
    "    lay = Convolution2D(filters=56,kernel_size=(3,3),strides=(1,1),padding='same')(lay)\n",
    "    lay = BatchNormalization()(lay)\n",
    "    lay = Activation('relu')(lay)\n",
    "    lay = AveragePooling2D(pool_size=(2,2),strides=2,padding='same')(lay)\n",
    "    lay = Dropout(0.3)(lay)\n",
    "\n",
    "    lay = Convolution2D(filters=72,kernel_size=(3,3),strides=(1,1),padding='same')(lay)\n",
    "    lay = BatchNormalization()(lay)\n",
    "    lay = Activation('relu')(lay)\n",
    "    lay = AveragePooling2D(pool_size=(2,2),strides=2,padding='same')(lay)\n",
    "    lay = Dropout(0.3)(lay)\n",
    "\n",
    "    lay = Flatten()(lay)\n",
    "\n",
    "    lay = Dense(48)(lay)\n",
    "    lay = BatchNormalization()(lay)\n",
    "    lay = Activation('relu')(lay)\n",
    "    lay = Dropout(0.15)(lay)\n",
    "    lay = Dense(n_classes)(lay)\n",
    "    # supervised output\n",
    "    c_out_layer = Activation('softmax')(lay)\n",
    "    # define and compile supervised discriminator model\n",
    "    c_model = Model(in_image, c_out_layer)\n",
    "    c_model.compile(\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        optimizer=Adam(lr=0.003, beta_1=0.5), \n",
    "        metrics=['accuracy'])\n",
    " \n",
    "    \n",
    "    # unsupervised output\n",
    "    d_out_layer = Lambda(custom_activation)(lay)\n",
    "    # define and compile unsupervised discriminator model\n",
    "    d_model = Model(in_image, d_out_layer)\n",
    "    d_model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "    return d_model, c_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create model\n",
    "# d_model, c_model = define_discriminator()\n",
    "# # plot the model\n",
    "# c_model.summary()\n",
    "# d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 28 * 42\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((28, 42, 128))(gen)\n",
    "    # upsample to double -56*84\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    # upsample to double- 112*168\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    # output\n",
    "    out_layer = Conv2D(3, (7,7), activation='tanh', padding='same')(gen)\n",
    "    # define model\n",
    "    model = Model(in_lat, out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # size of the latent space\n",
    "# latent_dim = 100\n",
    "\n",
    "# g_model = define_generator(latent_dim)\n",
    "# g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect image output from generator as input to discriminator\n",
    "    gan_output = d_model(g_model.output)\n",
    "    # define gan model as taking noise and outputting a classification\n",
    "    model = Model(g_model.input, gan_output)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a supervised subset of the dataset, ensures classes are balanced\n",
    "def select_supervised_samples(dataset, n_samples=20, n_classes=9):\n",
    "    X, y = dataset\n",
    "    y = np.argmax(y, axis=1)\n",
    "    X_list, y_list = list(), list()\n",
    "    n_per_class = int(n_samples / n_classes)\n",
    "    for i in range(n_classes):\n",
    "        # get all images for this class\n",
    "        X_with_class = X[y == i]\n",
    "        # choose random instances\n",
    "        ix = randint(0, len(X_with_class), n_per_class)\n",
    "        # add to list\n",
    "        [X_list.append(X_with_class[j]) for j in ix]\n",
    "        [y_list.append(i) for j in ix]\n",
    "    return asarray(X_list), asarray(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# split into images and labels\n",
    "\timages, labels = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, images.shape[0], n_samples)\n",
    "\t# select images and labels\n",
    "\tX, labels = images[ix], labels[ix]\n",
    "\t# generate class labels\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tz_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tz_input = z_input.reshape(n_samples, latent_dim)\n",
    "\treturn z_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tz_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\timages = generator.predict(z_input)\n",
    "\t# create class labels\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn images, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, c_model, latent_dim, dataset, n_samples=20):\n",
    "#     # prepare fake examples\n",
    "#     X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "#     # scale from [-1,1] to [0,1]\n",
    "#     X = (X + 1) / 2.0\n",
    "#     # plot images\n",
    "#     for i in range(100):\n",
    "#         # define subplot\n",
    "#         pyplot.subplot(10, 10, 1 + i)\n",
    "#         # turn off axis\n",
    "#         pyplot.axis('off')\n",
    "#         # plot raw pixel data\n",
    "#         pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "#     # save plot to file\n",
    "#     filename1 = 'generated_plot_%04d.png' % (step+1)\n",
    "#     pyplot.savefig(filename1)\n",
    "#     pyplot.close()\n",
    "    # evaluate the classifier model\n",
    "    X, y = dataset\n",
    "    y = np.argmax(y, axis=1)\n",
    "    _, acc = c_model.evaluate(X, y, verbose=0)\n",
    "    print('Classifier Accuracy: %.3f%%' % (acc * 100))\n",
    "    # save the generator model\n",
    "    filename2 = 'g_model_%04d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    # save the classifier model\n",
    "    filename3 = 'c_model_%04d.h5' % (step+1)\n",
    "    c_model.save(filename3)\n",
    "#     print('>Saved: %s, %s, and %s' % (filename1, filename2, filename3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, c_model, gan_model, dataset, latent_dim, n_epochs=20, n_batch=8, verbose = False):\n",
    "    # select supervised dataset\n",
    "    X_sup, y_sup = select_supervised_samples(dataset)\n",
    "    print(X_sup.shape, y_sup.shape)\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    print('n_epochs=%d, n_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (n_epochs, n_batch, half_batch, bat_per_epo, n_steps))\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # update supervised discriminator (c)\n",
    "        [Xsup_real, ysup_real], _ = generate_real_samples([X_sup, y_sup], half_batch)\n",
    "        c_loss, c_acc = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "        # update unsupervised discriminator (d)\n",
    "        [X_real, _], y_real = generate_real_samples(dataset, half_batch)\n",
    "        d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "        # update generator (g)\n",
    "        X_gan, y_gan = generate_latent_points(latent_dim, n_batch), ones((n_batch, 1))\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        # summarize loss on this batch\n",
    "        if(verbose):\n",
    "            print('>%d, c[%.3f,%.0f], d[%.3f,%.3f], g[%.3f]' % (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
    "        # evaluate the model performance every so often\n",
    "        if (i+1) % (bat_per_epo * 1) == 0:\n",
    "            summarize_performance(i, g_model, c_model, latent_dim, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 112, 168, 3) (18,)\n",
      "n_epochs=20, n_batch=8, 1/2=4, b/e=342, steps=6840\n",
      "Classifier Accuracy: 18.905%\n",
      "Classifier Accuracy: 31.606%\n",
      "Classifier Accuracy: 15.511%\n",
      "Classifier Accuracy: 26.058%\n",
      "Classifier Accuracy: 24.635%\n",
      "Classifier Accuracy: 25.657%\n",
      "Classifier Accuracy: 20.328%\n",
      "Classifier Accuracy: 38.321%\n",
      "Classifier Accuracy: 25.693%\n",
      "Classifier Accuracy: 24.891%\n",
      "Classifier Accuracy: 32.810%\n",
      "Classifier Accuracy: 22.336%\n",
      "Classifier Accuracy: 31.898%\n",
      "Classifier Accuracy: 33.102%\n",
      "Classifier Accuracy: 25.730%\n",
      "Classifier Accuracy: 26.131%\n",
      "Classifier Accuracy: 25.073%\n",
      "Classifier Accuracy: 24.781%\n",
      "Classifier Accuracy: 26.095%\n",
      "Classifier Accuracy: 29.635%\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator models\n",
    "d_model, c_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = [X_train, Y_train]\n",
    "# train model\n",
    "train(g_model, d_model, c_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "n_split=10\n",
    "n_classes=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_base_precision = list()\n",
    "all_base_recall = list()\n",
    "all_macro_precision = list()\n",
    "all_macro_recall = list()\n",
    "all_f1 = list()\n",
    "all_macro_f1 = list()\n",
    "\n",
    "all_tpr = list()\n",
    "all_fpr = list()\n",
    "all_precision = list()\n",
    "all_recall = list()\n",
    "all_cm=list()\n",
    "all_accuracy = list()\n",
    "\n",
    "all_df = list()\n",
    "\n",
    "y_all_labelled=[]\n",
    "for i in range(len(Y_all)):\n",
    "        y_all_labelled.append(np.argmax(Y_all[i]))\n",
    "             \n",
    "count = 1\n",
    "\n",
    "\n",
    "for train_index,test_index in StratifiedKFold(n_split, shuffle=True, random_state=123).split(X_all,y_all_labelled):  \n",
    "    # use the index to generate training an testing sets\n",
    "    x_train,x_test=X_all[train_index],X_all[test_index]\n",
    "    y_train,y_test=Y_all[train_index],Y_all[test_index]\n",
    "    \n",
    "    \n",
    "    file_name = mode_version +'_iter_' +str(count)\n",
    "\n",
    "    #Load History \n",
    "    hist_json_file = file_name + '.json'\n",
    "    history_info = pd.read_json(hist_json_file)\n",
    "    all_df.append(history_info)\n",
    "    \n",
    "   \n",
    "    # Load Model\n",
    "    model_file = file_name + '.model'\n",
    "    print('File Name Loaded: ',model_file)\n",
    "    cv_model = tf.keras.models.load_model(model_file) \n",
    "    \n",
    "    count+=1\n",
    "\n",
    "    #generate predictions\n",
    "    y_pred = cv_model.predict(x_test)\n",
    "    actual, predicted = generate_actual_predicted(y_pred, x_test, y_test)\n",
    "    \n",
    "    #calc metrics\n",
    "    curr_base_prec, curr_base_rec, curr_f1, _ = precision_recall_fscore_support(actual, predicted)\n",
    "    curr_macro_prec = precision_score(actual, predicted,average='macro')\n",
    "    curr_macro_rec = recall_score(actual, predicted,average='macro')\n",
    "    curr_macro_f1 = f1_score(actual, predicted,average='macro')\n",
    "    \n",
    "    actual_labeled = list()\n",
    "    predict_labeled = list()\n",
    "    for x,y in zip(actual,predicted):\n",
    "        actual_labeled.append(getSpecies(x))\n",
    "        predict_labeled.append(getSpecies(y))\n",
    "        \n",
    "    labels = ['Not_Bat', 'Bat']\n",
    "    curr_cm=confusion_matrix(actual_labeled,predict_labeled,labels=labels)\n",
    "    \n",
    "    curr_fpr = [0] * n_classes\n",
    "    curr_tpr = [0] * n_classes\n",
    "    for i in range(n_classes):\n",
    "        curr_fpr[i], curr_tpr[i], _ = roc_curve(y_test[:,i], y_pred[:,i])\n",
    "        \n",
    "    curr_prec = [0] * n_classes\n",
    "    curr_rec = [0] * n_classes\n",
    "    for i in range(n_classes):\n",
    "        curr_prec[i], curr_rec[i], _ = precision_recall_curve(y_test[:,i], y_pred[:,i])\n",
    "\n",
    "        \n",
    "    curr_accuracy = accuracy_score(actual, predicted)\n",
    "    \n",
    "    #add to lists\n",
    "    all_base_precision.append(curr_base_prec)\n",
    "    all_base_recall.append(curr_base_rec)\n",
    "    all_macro_precision.append(curr_macro_prec)\n",
    "    all_macro_recall.append(curr_macro_rec)\n",
    "    \n",
    "    all_f1.append(curr_f1)\n",
    "    all_macro_f1.append(curr_macro_f1)\n",
    "    \n",
    "    all_accuracy.append(curr_accuracy)\n",
    "\n",
    "    \n",
    "    all_fpr.append(curr_fpr)\n",
    "    all_tpr.append(curr_tpr)\n",
    "    all_precision.append(curr_prec)\n",
    "    all_recall.append(curr_rec)\n",
    "    all_cm.append(curr_cm)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in all_df:\n",
    "    _, axs = plt.subplots(ncols=2)\n",
    "\n",
    "    sns.lineplot(data=df['loss'],ax=axs[0], label='Training Loss', legend='brief')\n",
    "    sns.lineplot(data=df['val_loss'],ax=axs[0], label='Validation Loss', legend='brief')\n",
    "\n",
    "    sns.lineplot(data=df['accuracy'],ax=axs[1], label='Training Accuracy', legend='brief')\n",
    "    sns.lineplot(data=df['val_accuracy'],ax=axs[1], label='Validation Accuracy', legend='brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('precision mean by class', np.array(all_base_precision).mean(axis=0))\n",
    "print('precision sdev by class', np.array(all_base_precision).std(axis=0))\n",
    "\n",
    "print('macro precision mean', np.array(all_macro_precision).mean())\n",
    "print('macro precision sdev', np.array(all_macro_precision).std())\n",
    "\n",
    "\n",
    "print('recall mean', np.array(all_base_recall).mean(axis=0))\n",
    "print('recall sdev', np.array(all_base_recall).std(axis=0))\n",
    "\n",
    "print('macro recall mean', np.array(all_macro_recall).mean())\n",
    "print('macro recall sdev', np.array(all_macro_recall).std())\n",
    "\n",
    "print('f1 mean', np.array(all_f1).mean(axis=0))\n",
    "print('f1 sdev', np.array(all_f1).std(axis=0))\n",
    "\n",
    "print('macro f1 mean', np.array(all_macro_f1).mean())\n",
    "print('macro f1 sdev', np.array(all_macro_f1).std())\n",
    "\n",
    "print('accuracy mean', np.array(all_accuracy).mean())\n",
    "print('accuracy sdev', np.array(all_accuracy).std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all K-Fold ROC curves\n",
    "\n",
    "for i in range(n_split):\n",
    "    _, axs = plt.subplots(ncols=1)\n",
    "    axs.set(xlabel='False Positive Rate',ylabel='True Positive Rate', title='ROC Curve K-Fold#'+str(i+1))\n",
    "    for j in range(n_classes):\n",
    "        auc_val = auc(all_fpr[i][j], all_tpr[i][j])\n",
    "        auc_val = np.around(auc_val,4)\n",
    "        sns.lineplot(x=all_fpr[i][j],y=all_tpr[i][j],ax=axs, label='Class '+ getSpecies(j) +' (area = ' + str(auc_val) + ')', legend='brief')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs = list()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    aucs.append(list())\n",
    "\n",
    "\n",
    "# Plot all K-Fold ROC curves\n",
    "\n",
    "for i in range(n_split):\n",
    "    for j in range(n_classes):\n",
    "        auc_val = auc(all_fpr[i][j], all_tpr[i][j])\n",
    "        aucs[j].append(auc_val)\n",
    "        \n",
    "for i in range(n_classes):\n",
    "    print(\"Sp: \", getSpecies(i))\n",
    "    print(\"AUC Mean \", np.array(aucs[i]).mean())\n",
    "    print(\"Std \", np.array(aucs[i]).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all K-Fold Precision/Recall curves\n",
    "for i in range(n_split):\n",
    "    _, axs = plt.subplots(ncols=1)\n",
    "    axs.set(xlabel='Recall',ylabel='Precision', title='Precision/Recall Curve K-Fold#'+str(i+1))\n",
    "    for j in range(n_classes):\n",
    "        sns.lineplot(x=all_recall[i][j],y=all_precision[i][j],ax=axs, label='Class '+ getSpecies(j), legend='brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot all K-Fold confusion matrices\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create 1X 5 sub plots\n",
    "# gs = gridspec.GridSpec(1,n_split)\n",
    "figs=[]\n",
    "for i in range(n_split):\n",
    "    figs.append(plt.figure())\n",
    "    \n",
    "for i in range(n_split):\n",
    "    print('Confusion Matrix K-Fold #'+ str(i+1)+\"\\n\")\n",
    "    print(all_cm[i])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ax=figs[i].add_subplot()\n",
    "    sns.heatmap(all_cm[i], annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "    \n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix K-Fold #'+ str(i+1)); \n",
    "    ax.xaxis.set_ticklabels(labels,rotation=45); ax.yaxis.set_ticklabels(labels,rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot macro cm\n",
    "\n",
    "sum_all_cm=np.zeros((n_classes,n_classes)).astype('int64')\n",
    "for i in range(n_split):\n",
    "    sum_all_cm+=all_cm[i]\n",
    "    \n",
    "    \n",
    "#plot \n",
    "figs=[]\n",
    "for i in range(2):\n",
    "    figs.append(plt.figure())\n",
    "       \n",
    "print(\"sum of all confuion matrices\\n\",sum_all_cm)\n",
    "ax=figs[0].add_subplot()\n",
    "sns.heatmap(sum_all_cm, annot=True, ax = ax);\n",
    "\n",
    "avg_all_cm=np.divide(sum_all_cm,n_split).astype('int64')\n",
    "print(\"\\naverage of all confuion matrices\\n\",avg_all_cm)\n",
    "ax=figs[1].add_subplot()\n",
    "sns.heatmap(avg_all_cm, annot=True, ax = ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "cm= sum_all_cm/ sum_all_cm.astype(np.float).sum(axis=1,keepdims = True)\n",
    "cm = (np.around(cm,2))\n",
    "print(cm)\n",
    "\n",
    "\n",
    "labels = ['Not_Bat', 'Bat']\n",
    "\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(labels,rotation=45); ax.yaxis.set_ticklabels(labels,rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
